# -*- coding: utf-8 -*-
"""PNN model training_JULIA SUMMERCAMP PYTHON CODE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g22tfDr4g98n0YFo_CPB8BgF6SjAJnLg
"""

import torch  # to use PyTorch (optimized tensor library for deep learning using GPU and CPU)
from torch import nn  #module torch. nn: diff classes, help  build nns
from torch.utils.data import DataLoader, Dataset
import numpy as np   # NumPy: Python library used for working w/arrays
                     #np.arrange, np.random.shuffle
import pandas as pd  #pandas: software library for data manipulation + analysis
                     #pd.read.csv   
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import torch.optim as optim
import math
from sklearn.model_selection import train_test_split
from scipy.linalg import norm

torch.cuda.is_available()
from google.colab import drive   #importing drive
drive.mount('/content/drive')

pip install -U --no-cache-dir gdown --pre

#download the data from google drive to colab and read them
! gdown --id 1WJVQOxcO1b4jAPlAnh1lW_X_6umiJxI4
df1 = pd.read_csv("parameters.csv",header=None)
print(df1.head()) #gives first 5 rows
print(df1.shape) #prints number of rows

! gdown --id 13N_-Eqw1JjhwP8uuEmx_fWqbNYzosoJ5
df2 = pd.read_csv("real_t.csv",header=None)
print(df2.head()) #gives first 5 rows
print(df2.shape) #prints number of rows

! gdown --id 1LVX7n6qRWETKNr5FLxd015fYk86MTMDA
df3 = pd.read_csv("imag_t.csv",header=None)
print(df3.head()) #gives first 5 rows
print(df3.shape) #prints number of rows

#! gdown --id 1O_jaMlEIqOsWnRdEZeDElIRmm7LUEbEB
#sinf = pd.read_csv("sinf.csv",header=None)
#print(sinf.head()) #gives first 5 rows
#print(sinf.shape) #prints number of rows

#! gdown --id 1-3PyYjZPN4qaC4DG9P-s9MphrbasPIE-
#dualf = pd.read_csv("dualf.csv",header=None)
#print(dualf.head()) #gives first 5 rows
#print(dualf.shape) #prints number of rows
 
#sinf=torch.tensor(sinf.iloc[0:len(sinf)].values).float().cuda()   #to tensor on cuda
#dualf=torch.tensor(dualf.iloc[0:len(dualf)].values).float().cuda()    #to tensor on cuda
x_full=torch.tensor(df1.iloc[0:len(df1)].values).float().cuda()   #full x data from parameters.csv
real_full=torch.tensor(df2.iloc[0:len(df2)].values).float().cuda()   #full y data from real_t
imag_full=torch.tensor(df3.iloc[0:len(df3)].values).float().cuda()   #full y data from imag_t
amplitude_full = torch.sqrt(torch.mul(real_full, real_full) +torch.mul(imag_full, imag_full))

#separate the data sets into training and test
 # parameters
dropout_rate = 0.7
td_ratio = 1-dropout_rate #percent training data
batch_size = 200

x_full=torch.tensor(df1.iloc[0:len(df1)].values).float().cuda()   #full x data from parameters.csv
real_full=torch.tensor(df2.iloc[0:len(df2)].values).float().cuda()   #full y data from real_t
imag_full=torch.tensor(df3.iloc[0:len(df3)].values).float().cuda()   #full y data from imag_t

x_train, x_test, real_train, real_test , imag_train, imag_test = train_test_split(x_full,real_full, imag_full, test_size=td_ratio, random_state=42)
#print real data shapes 
print('full x data shape:',x_full.shape)
print('x train data shape:', x_train.shape)
print('x test data shape:', x_test.shape)
print('real full data shape:',  real_full.shape)
print('real train data shape:', real_train.shape)
print('real test data shape:', real_test.shape)
print('imag full data shape:',  imag_full.shape)
print('imag train data shape:', imag_train.shape)
print('imag test data shape:', imag_test.shape)
#Define our Dataset Class and create dataloader for real and imag

class SpectroData(Dataset):

  def __init__(self, data): #load data and convert to tensors
    self.data = data
  def __getitem__(self, index): #gives data from imported dataset
    return self.data[index] 

  def __len__(self):  #returns length of tensor
    return len(self.data)

#combine train data for realdataloader
train_dataset1= torch.utils.data.TensorDataset(x_train, real_train)   
test_dataset1= torch.utils.data.TensorDataset(x_test, real_test)   

train_dataset_real=SpectroData(train_dataset1)
test_dataset_real=SpectroData(test_dataset1)
 
#load x_train and real_train together
train_loader_real = torch.utils.data.DataLoader(train_dataset_real, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False) 
test_loader_real = torch.utils.data.DataLoader(test_dataset_real, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False) 


#combine train data for imag dataloader
train_dataset2= torch.utils.data.TensorDataset(x_train, imag_train)   
test_dataset2= torch.utils.data.TensorDataset(x_test, imag_test)   

train_dataset_imag=SpectroData(train_dataset2)
test_dataset_imag=SpectroData(test_dataset2)
 
#load x_train and imag_train together
train_loader_imag = torch.utils.data.DataLoader(train_dataset_imag, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False) 
test_loader_imag= torch.utils.data.DataLoader(test_dataset_imag, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)

#define PNN model and load the trained PNN model

# the f function in the papter(calculate tensor product using einsum)
def f(x, W1,W2, V, b): #def bilinear function; x is a N*1*4
  n=len(x)

  xsq = torch.transpose(torch.mul(x, x),1,0)  #squares x
  xt=torch.transpose(x, 1,0)

  xT_W1 =torch.einsum("mj,ijk",x,W1).permute(0,2,1).cuda()
  xT_W1_x=torch.transpose(torch.diagonal(torch.einsum("ijk,km", xT_W1,xt), offset=0, dim1=1, dim2=2),1,0) #n*50
  xT_W2 =torch.einsum("mj,ijk",x,W2).permute(0,2,1).cuda()
  xT_W2_x=torch.transpose(torch.diagonal(torch.einsum("ijk,km", xT_W2,xsq), offset=0, dim1=1, dim2=2),1,0) #n*50
  V_xcat= torch.transpose(torch.mm(V,torch.cat((xt, xsq))),1,0)
  b1=torch.transpose(b.repeat(1,n),1,0)
  Q= xT_W1_x+xT_W2_x+V_xcat+b1
  return(Q)  

#define the PNN model

class NeuralNetwork(nn.Module):   #https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.params = nn.ParameterDict({
                'W1': nn.Parameter(torch.ones(50, 4, 4).cuda(), requires_grad = True),
                'W2': nn.Parameter(torch.ones(50, 4, 4).cuda(), requires_grad = True),
                'V':  nn.Parameter(torch.ones(50,8).cuda(), requires_grad = True),
                'b':  nn.Parameter(torch.ones(50,1).cuda(), requires_grad = True)

        })   
        self.linear_relu_stack = nn.Sequential(   #nn.Sequential -> output of each layer as input of next layer
            nn.ReLU(),  #ReLU: activation function (defines output/what goes into next layer)
            nn.Linear(50, 500), # creates single layer feed forward network with 50 inputs and 500 outputs
            nn.ReLU(),
            nn.Linear(500, 500),
            nn.ReLU(),
            nn.Linear(500, 500),
            nn.ReLU(),
            nn.Linear(500, 301),

        )
 
    
    def forward(self, x): #def forward function
       Q=f(x, self.params['W1'],self.params['W2'],self.params['V'], self.params['b'])
       logits = self.linear_relu_stack(Q)
       return logits
       

PNN_real = NeuralNetwork().cuda()    #define model

#define save function to save checkpoint 
# after each iteration, the trained model, and results are saved, and will be loaded in the next iteration 
import shutil
def save_ckp(state, checkpoint_path):
    f_path = checkpoint_path
    torch.save(state, f_path)
    
def load_ckp(checkpoint_fpath, model, optimizer):
    checkpoint = torch.load(checkpoint_fpath)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  #  valid_loss_min = checkpoint['valid_loss_min']
    return model, optimizer, checkpoint['epoch'], checkpoint['Rsquare'],  checkpoint['TRAIN_LOSS'], checkpoint['TEST_LOSS'],checkpoint['inx']


#load the trained PNN models
current_checkpoint_PNN_real="/content/drive/MyDrive/current_checkpoint_PNN_real.pth" 
#model, optimizer, start_epoch, r_squared,Trainloss,Testloss,indices = load_ckp(current_checkpoint, model, optimizer)
lr = 1e-06
PNN_real = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_real.parameters(), lr = lr)

#define the function to train the model using real train data
def PNN_real_train(start_epoch, num_epoch, model, optimizer, checkpoint_path):
    print("learning rate", lr)  
    # initialize tracker for minimum validation loss
  #  valid_loss_min = valid_loss_min_input 
    for epoch in range(start_epoch, num_epoch+start_epoch):
        # initialize variables to monitor training and validation loss
        #train_loss = 0.0
        
        PNN_real.train()
        for batch_idx, (x_train_batch, y_train_batch) in enumerate(train_loader_real):
            optimizer.zero_grad()   #optimize after each batch
            x_train_batch, y_train_batch =(Variable(x_train_batch),Variable(y_train_batch))
            y_pred_batch = PNN_real(x_train_batch)
       #combine all y_pred_batch into y_pred
            if batch_idx==0: y_pred_train=  y_pred_batch
            else:            y_pred_train = torch.cat((y_pred_train, y_pred_batch),0)
            loss =loss_func(y_train_batch, y_pred_batch)
            loss.backward(retain_graph=True)
            optimizer.step()
 
        
        loss_train =loss_func(y_pred_train, real_train.cuda()) # calculate loss of train data for each epoch
        Trainloss.append(loss_train.item())  # Compute and print loss
        #if epoch%100==1: print("trainloss:", Trainloss)

        PNN_real.eval() #not optimize, for each epoch: test loss
        with torch.no_grad(): # stop updating the parameters
          for batch_idx, (x_test_batch, y_test_batch) in enumerate(test_loader_real):
              x_test_batch, y_test_batch =(Variable(x_test_batch),Variable(y_test_batch))
              y_pred_batch = PNN_real(x_test_batch)
              if batch_idx==0: 
                  y_pred_test=  y_pred_batch
              else:
                  y_pred_test = torch.cat((y_pred_test, y_pred_batch),0)
      # prediction_train = model(x_train)
          loss_test = loss_func(y_pred_test,real_test.cuda())
          Testloss.append(loss_test.item())  # Compute and print loss
  

        indices.append(epoch+1)    #record epoch

        y_train1=real_train.reshape(len(real_train),301).cuda()
        y_train2 = y_train1.detach().cuda()
        y_pred1=y_pred_train.reshape(len(real_train),301)
        y_pred2 = y_pred1.detach()
        r2=r2_score(y_train2.cpu(), y_pred2.cpu())  #y_train2: real_train, y_pred2: y_pred
        r_squared.append(r2)

        if epoch%100==1: 
          print('epoch', epoch)
          print("TRAIN_LOSS", Trainloss[-5:])
          print("TEST_LOSS",Testloss[-5:])
          print("Rsquare",r_squared[-5:])
        checkpoint = {
            'epoch': epoch + 1,
            'Rsquare': r_squared,
            'TRAIN_LOSS': Trainloss,
            'TEST_LOSS': Testloss,
            'inx':indices,
            #'valid_loss_min': valid_loss,
            'state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }
        if epoch%100==1 or epoch==num_epoch+start_epoch-1: save_ckp(checkpoint, current_checkpoint_PNN_real) # save the model checkpoint every 100 iterations
        
            
    # return trained model
    return PNN_real

#before training the model
from torch.autograd import Variable  # Variable wraps tensor, gives way to perform backpropagation
lr = 1e-05
loss_func=nn.MSELoss()
PNN_real = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_real.parameters(), lr = lr)
current_checkpoint_PNN_real="/content/drive/MyDrive/current_checkpoint_PNN_real.pth"

#initiate the training
#DONT RUN this cell if you want to continue the contraining.
#If this is the first time of training, start_epoch=1, otherwise go to the next cell
#start_epoch=1
if (start_epoch==1):
    Trainloss=[] # loss of every epoch in iteration for training data
    Testloss = [] #loss of every epoch in iteration for test data
    r_squared = []
    indices = []  #epochs
    y_pred = []
    y_pred_test=[]
    lr = lr
    start_epoch=1 
    num_epoch=1
    trained_model = PNN_real_train(start_epoch, num_epoch, PNN_real, optimizer, current_checkpoint_PNN_real) #train the model and save the checkpoint
 
else:

    print('start epoch:',start_epoch) 
    #print(model)
    #print(optimizer)
    #print(start_epoch) 
    print(Trainloss[-5:])
    print(Testloss[-5:])
    print(indices[-5:])
    print(r_squared[-5:])
    num_epoch=1
    trained_model = PNN_real_train(start_epoch, num_epoch, PNN_real, optimizer, current_checkpoint_PNN_real) #train the model and save the checkpoint

#before training the model
from torch.autograd import Variable  # Variable wraps tensor, gives way to perform backpropagation
lr = 1e-05
loss_func=nn.MSELoss()
PNN_real = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_real.parameters(), lr = lr)
current_checkpoint_PNN_real="/content/drive/MyDrive/current_checkpoint_PNN_real.pth" 

 
PNN_real, optimizer, start_epoch, r_squared,Trainloss,Testloss,indices = load_ckp(current_checkpoint_PNN_real, PNN_real, optimizer)
num_epoch=30000
import time
start = time.time()
trained_model = PNN_real_train(start_epoch, num_epoch, PNN_real, optimizer, current_checkpoint_PNN_real) #train the model and save the checkpoint
end = time.time()
print("running time for PNN_real", end - start)

#define PNN_imag model
# the f function in the papter(calculate tensor product using einsum)
def f(x, W1,W2, V, b): #def bilinear function; x is a N*1*4
  n=len(x)

  xsq = torch.transpose(torch.mul(x, x),1,0)  #squares x
  xt=torch.transpose(x, 1,0)

  xT_W1 =torch.einsum("mj,ijk",x,W1).permute(0,2,1).cuda()
  xT_W1_x=torch.transpose(torch.diagonal(torch.einsum("ijk,km", xT_W1,xt), offset=0, dim1=1, dim2=2),1,0) #n*50
  xT_W2 =torch.einsum("mj,ijk",x,W2).permute(0,2,1).cuda()
  xT_W2_x=torch.transpose(torch.diagonal(torch.einsum("ijk,km", xT_W2,xsq), offset=0, dim1=1, dim2=2),1,0) #n*50
  V_xcat= torch.transpose(torch.mm(V,torch.cat((xt, xsq))),1,0)
  b1=torch.transpose(b.repeat(1,n),1,0)
  Q= xT_W1_x+xT_W2_x+V_xcat+b1
  return(Q)  

 
class NeuralNetwork(nn.Module):   #https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.params = nn.ParameterDict({
                'W1': nn.Parameter(torch.ones(50, 4, 4).cuda(), requires_grad = True),
                'W2': nn.Parameter(torch.ones(50, 4, 4).cuda(), requires_grad = True),
                'V':  nn.Parameter(torch.ones(50,8).cuda(), requires_grad = True),
                'b':  nn.Parameter(torch.ones(50,1).cuda(), requires_grad = True)

        })   
        self.linear_relu_stack = nn.Sequential(   #nn.Sequential -> output of each layer as input of next layer
            nn.ReLU(),  #ReLU: activation function (defines output/what goes into next layer)
            nn.Linear(50, 500), # creates single layer feed forward network with 50 inputs and 500 outputs
            nn.ReLU(),
            nn.Linear(500, 500),
            nn.ReLU(),
            nn.Linear(500, 500),
            nn.ReLU(),
            nn.Linear(500, 301),

        )
 
    
    def forward(self, x): #def forward function
       Q=f(x, self.params['W1'],self.params['W2'],self.params['V'], self.params['b'])
       logits = self.linear_relu_stack(Q)
       return logits
       

PNN_imag = NeuralNetwork().cuda()    #define model

#define save function to save checkpoint 
# after each iteration, the trained model, and results are saved, and will be loaded in the next iteration 
import shutil
def save_ckp(state, checkpoint_path):
    f_path = checkpoint_path
    torch.save(state, f_path)
    
def load_ckp(checkpoint_fpath, model, optimizer):
    checkpoint = torch.load(checkpoint_fpath)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  #  valid_loss_min = checkpoint['valid_loss_min']
    return model, optimizer, checkpoint['epoch'], checkpoint['Rsquare'],  checkpoint['TRAIN_LOSS'], checkpoint['TEST_LOSS'],checkpoint['inx']


#load the trained PNN models
current_checkpoint_PNN_real="/content/drive/MyDrive/current_checkpoint_PNN_imag.pth" 
#model, optimizer, start_epoch, r_squared,Trainloss,Testloss,indices = load_ckp(current_checkpoint, model, optimizer)
lr = 1e-05
PNN_imag = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_imag.parameters(), lr = lr)

#define the function to train the model using imag train data
def PNN_imag_train(start_epoch, num_epoch, model, optimizer, checkpoint_path):
    print("learning rate", lr)  
    # initialize tracker for minimum validation loss
  #  valid_loss_min = valid_loss_min_input 
    for epoch in range(start_epoch, num_epoch+start_epoch):
        # initialize variables to monitor training and validation loss
        #train_loss = 0.0
        
        PNN_imag.train()
        for batch_idx, (x_train_batch, y_train_batch) in enumerate(train_loader_imag):
            optimizer.zero_grad()   #optimize after each batch
            x_train_batch, y_train_batch =(Variable(x_train_batch),Variable(y_train_batch))
            y_pred_batch = PNN_imag(x_train_batch)
       #combine all y_pred_batch into y_pred
            if batch_idx==0: y_pred_train=  y_pred_batch
            else:            y_pred_train = torch.cat((y_pred_train, y_pred_batch),0)
            loss =loss_func(y_train_batch, y_pred_batch)
            loss.backward(retain_graph=True)
            optimizer.step()
 
        
        loss_train =loss_func(y_pred_train, imag_train.cuda()) # calculate loss of train data for each epoch
        Trainloss.append(loss_train.item())  # Compute and print loss
        #if epoch%100==1: print("trainloss:", Trainloss)

        PNN_imag.eval() #not optimize, for each epoch: test loss
        with torch.no_grad(): # stop updating the parameters
          for batch_idx, (x_test_batch, y_test_batch) in enumerate(test_loader_real):
              x_test_batch, y_test_batch =(Variable(x_test_batch),Variable(y_test_batch))
              y_pred_batch = PNN_imag(x_test_batch)
              if batch_idx==0: 
                  y_pred_test=  y_pred_batch
              else:
                  y_pred_test = torch.cat((y_pred_test, y_pred_batch),0)
      # prediction_train = model(x_train)
          loss_test = loss_func(y_pred_test,imag_test.cuda())
          Testloss.append(loss_test.item())  # Compute and print loss
  

        indices.append(epoch+1)    #record epoch

        y_train1=imag_train.reshape(len(imag_train),301).cuda()
        y_train2 = y_train1.detach().cuda()
        y_pred1=y_pred_train.reshape(len(imag_train),301)
        y_pred2 = y_pred1.detach()
        r2=r2_score(y_train2.cpu(), y_pred2.cpu())  #y_train2: real_train, y_pred2: y_pred
        r_squared.append(r2)

        if epoch%100==1: 
          print('epoch', epoch)
          print("TRAIN_LOSS", Trainloss[-5:])
          print("TEST_LOSS",Testloss[-5:])
          print("Rsquare",r_squared[-5:])
        checkpoint = {
            'epoch': epoch + 1,
            'Rsquare': r_squared,
            'TRAIN_LOSS': Trainloss,
            'TEST_LOSS': Testloss,
            'inx':indices,
            #'valid_loss_min': valid_loss,
            'state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }
        if epoch%100==1 or epoch==num_epoch+start_epoch-1: save_ckp(checkpoint, current_checkpoint_PNN_imag) # save the model checkpoint every 100 iterations
        
            
    # return trained model
    return PNN_imag

#before training the model
from torch.autograd import Variable  # Variable wraps tensor, gives way to perform backpropagation
lr = 1e-05
loss_func=nn.MSELoss()
PNN_imag = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_imag.parameters(), lr = lr)
current_checkpoint_PNN_imag="/content/drive/MyDrive/current_checkpoint_PNN_imag.pth"

#initiate the training
#DONT RUN this cell if you want to continue the contraining.
#If this is the first time of training, start_epoch=1, otherwise go to the next cell
#start_epoch=1
if (start_epoch==1):
    Trainloss=[] # loss of every epoch in iteration for training data
    Testloss = [] #loss of every epoch in iteration for test data
    r_squared = []
    indices = []  #epochs
    y_pred = []
    y_pred_test=[]
    lr = lr
    start_epoch=1 
    num_epoch=1
    trained_model = PNN_imag_train(start_epoch, num_epoch, PNN_imag, optimizer, current_checkpoint_PNN_imag) #train the model and save the checkpoint
 
else:

    print('start epoch:',start_epoch) 
    #print(model)
    #print(optimizer)
    #print(start_epoch) 
    print(Trainloss[-5:])
    print(Testloss[-5:])
    print(indices[-5:])
    print(r_squared[-5:])
    num_epoch=1
    trained_model = PNN_imag_train(start_epoch, num_epoch, PNN_imag, optimizer, current_checkpoint_PNN_imag) #train the model and save the checkpoint

#before training the model
from torch.autograd import Variable  # Variable wraps tensor, gives way to perform backpropagation
lr = 1e-05
loss_func=nn.MSELoss()
PNN_imag = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_real.parameters(), lr = lr)
current_checkpoint_PNN_imag="/content/drive/MyDrive/current_checkpoint_PNN_imag.pth" 

 
PNN_imag, optimizer, start_epoch, r_squared,Trainloss,Testloss,indices = load_ckp(current_checkpoint_PNN_imag, PNN_real, optimizer)
num_epoch=30000
import time
start = time.time()
trained_model = PNN_imag_train(start_epoch, num_epoch, PNN_imag, optimizer, current_checkpoint_PNN_imag) #train the model and save the checkpoint
end = time.time()
print("running time", end - start)

#predict real and imag using the trainde PNN_real and PNN_imag models

x_full=torch.tensor(df1.iloc[0:len(df1)].values).float().cuda()   #full x data from parameters.csv
real_full=torch.tensor(df2.iloc[0:len(df2)].values).float().cuda()   #full y data from real_t
imag_full=torch.tensor(df3.iloc[0:len(df3)].values).float().cuda()   #full y data from imag_t
amplitude_full = torch.sqrt(torch.mul(real_full, real_full) +torch.mul(imag_full, imag_full))  


#Define our Dataset Class and create dataloader for real and imag

class SpectroData(Dataset):

  def __init__(self, data): #load data and convert to tensors
    self.data = data
  def __getitem__(self, index): #gives data from imported dataset
    return self.data[index] 

  def __len__(self):  #returns length of tensor
    return len(self.data)

dataset1= torch.utils.data.TensorDataset(x_full, real_full)   
dataset_real=SpectroData(dataset1)
dataloader_real = torch.utils.data.DataLoader(dataset_real, batch_size=200, shuffle=False, num_workers=0, pin_memory=False) 


lr = 1e-05
PNN_real = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_real.parameters(), lr = lr)
current_checkpoint_PNN_real="/content/drive/MyDrive/current_checkpoint_PNN_real.pth" 

PNN_real, optimizer, start_epoch, r_squared_real,Trainloss_real,Testloss_real,indices_real = load_ckp(current_checkpoint_PNN_real, PNN_real, optimizer)
PNN_real.eval() #not optimize, for each epoch: test loss
with torch.no_grad(): # stop updating the parameters
     for batch_idx, (x_batch, y_batch) in enumerate(dataloader_real):
         y_pred_batch = PNN_real(x_batch)
          #combine all y_pred_batch into y_pred_real
         if batch_idx==0: 
            pred_real=  y_pred_batch
         else:
            pred_real= torch.cat((pred_real, y_pred_batch),0)
pred_real_np=pred_real.cpu().numpy()
real_full_np=real_full.cpu().numpy()


dataset1= torch.utils.data.TensorDataset(x_full, imag_full)  
dataset_imag=SpectroData(dataset1)
dataloader_imag = torch.utils.data.DataLoader(dataset_imag, batch_size=200, shuffle=False, num_workers=0, pin_memory=False) 
lr = 1e-05
PNN_imag = NeuralNetwork().cuda() 
optimizer = optim.Adam(PNN_imag.parameters(), lr = lr)
current_checkpoint_PNN_imag="/content/drive/MyDrive/current_checkpoint_PNN_imag.pth" 
PNN_imag, optimizer, start_epoch, r_squared_imag,Trainloss_imag,Testloss_imag,indices_imag = load_ckp(current_checkpoint_PNN_imag, PNN_imag, optimizer)

 
PNN_imag, optimizer, start_epoch, r_squared_imag,Trainloss_imag,Testloss_imag,indices_imag = load_ckp(current_checkpoint_PNN_imag, PNN_imag, optimizer)
PNN_imag.eval() #not optimize, for each epoch: test loss
with torch.no_grad(): # stop updating the parameters
     for batch_idx, (x_batch, y_batch) in enumerate(dataloader_imag):
         y_pred_batch = PNN_imag(x_batch)
          #combine all y_pred_batch into y_pred_real
         if batch_idx==0: 
            pred_imag=  y_pred_batch
         else:
            pred_imag= torch.cat((pred_imag, y_pred_batch),0)
 
pred_imag_np=pred_imag.cpu().numpy()
imag_full_np=imag_full.cpu().numpy()



amplitude_full_np=np.square(real_full_np)+np.square(imag_full_np)
amplitude_pred_np=np.square(pred_real_np)+np.square(pred_imag_np)

#plot Real vs Predicted Real
#save the first 100 plots into one pdf file
from matplotlib.backends.backend_pdf import PdfPages
from google.colab import files
k=100  
frequency=np.arange(30, 60.1, 0.1) 
pdf = PdfPages('Real vs Predicted Real.pdf')
for i in range(100): 
        fig, ax = plt.subplots(figsize=(10, 10))
        plt.plot(frequency,real_full_np[i,], '-o')  # the ith row of all real data
        plt.plot(frequency,pred_real_np[i,], '-o')# the ith row of the predicted real data
        plt.xlabel('Frequency')
        plt.ylabel('Real')
        plt.legend(['Real','Predicted Real'])
        plt.title('Real vs Predicted Real_row %i'%(i+1))
        pdf.savefig(fig)
        plt.show()

files.download("Real vs Predicted Real.pdf") 

pdf.close()

#plot Imag vs Predicted Imag
#save the first 100 plots into one pdf file
from matplotlib.backends.backend_pdf import PdfPages
from google.colab import files

frequency=np.arange(30, 60.1, 0.1) 
pdf = PdfPages('Imag vs Predicted Imag.pdf')
for i in range(k): 
        fig, ax = plt.subplots(figsize=(10, 10))
        plt.plot(frequency,imag_full_np[i,], '-o')  # the ith row of all real data
        plt.plot(frequency,pred_imag_np[i,], '-o')# the ith row of the predicted real data
        plt.xlabel('Frequency')
        plt.ylabel('Imag')
        plt.legend(['Imag','Predicted Imag'])
        plt.title('Imag vs Predicted Imag_row %i'%(i+1))
        pdf.savefig(fig)
        plt.show()

#files.download("Imag vs Predicted Imag.pdf") 

pdf.close()

#plot amplitude vs Predicted amplitude
#save the first 100 plots into one pdf file
from matplotlib.backends.backend_pdf import PdfPages
from google.colab import files
amplitude_full_np=np.square(real_full_np)+np.square(imag_full_np)
amplitude_pred_np=np.square(pred_real_np)+np.square(pred_imag_np)
k=100
size=15
frequency=np.arange(30, 60.1, 0.1) 
pdf = PdfPages('Spectrum vs Predicted Spectrum.pdf')
for i in range(k): 
        fig, ax = plt.subplots(figsize=(10, 10))
        plt.plot(frequency,amplitude_full_np[i,], '-o')  # the ith row of all real data
        plt.plot(frequency,amplitude_pred_np[i,], '-o')# the ith row of the predicted real data
        plt.xlabel('Frequency',fontsize = size)
        plt.ylabel('Spectrum',fontsize = size)
        plt.legend(['Target Spectrum','Predicted Spectrum'],fontsize = size)
        plt.title('Target Spectrum vs Predicted Spectrum_row %i'%(i+1))
        pdf.savefig(fig)
        plt.show()

files.download("Spectrum vs Predicted Spectrum.pdf") 

pdf.close()

#plot the model loss
from matplotlib.backends.backend_pdf import PdfPages
from google.colab import files
pdf = PdfPages('PNN_Model Loss.pdf')
fig, ax = plt.subplots(figsize=(10, 10))
plt.yscale('log') #change the y-axis  scale to logarithmic
k=30000 # plot Model loss until kth iteration
size=15
plt.plot(Trainloss_real[0:k])
plt.plot(Testloss_real[0:k])
#plt.plot(Trainloss_imag[0:k])
#plt.plot(Testloss_imag[0:k])
plt.xlabel('Epoch',fontsize = size)
plt.ylabel('Log(Loss)',fontsize = size)
plt.legend(['Train Loss','Test Loss_real'],fontsize = size)
plt.title('PNN_Epoch vs Loss' )
#plt.savefig("Model Loss_Epoch="+str(k)+ ".png")
pdf.savefig(fig)
plt.show()
files.download("PNN_Model Loss.pdf") 
pdf.close()

#plot the model r-square
from matplotlib.backends.backend_pdf import PdfPages
from google.colab import files
pdf = PdfPages('PNN_Model R-square.pdf')
fig, ax = plt.subplots(figsize=(10, 10))
k=30000 # plot Model loss until kth iteration
size=15
plt.plot(r_squared_real[10:k])
plt.xlabel('Epoch',fontsize = size)
plt.ylabel('R-square',fontsize = size)
#plt.title('PNN_Model R-square_Epoch=%i'%k )
#plt.savefig("Model Loss_Epoch="+str(k)+ ".png")
pdf.savefig(fig)
plt.show()

files.download("PNN_Model R-square.pdf") 
pdf.close()