{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uio5uQQYTcRp"
   },
   "outputs": [],
   "source": [
    "import torch  # to use PyTorch (optimized tensor library for deep learning using GPU and CPU)\n",
    "from torch import nn  #module torch. nn: diff classes, help  build nns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np   # NumPy: Python library used for working w/arrays\n",
    "                     #np.arrange, np.random.shuffle\n",
    "import pandas as pd  #pandas: software library for data manipulation + analysis\n",
    "                     #pd.read.csv   \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/misha/Desktop/Python codes/Julia/training data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_QG8koT5zuL_"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(path + \"parameters.csv\", header=None)  #reads csv file; makes 0, 1, 2, 3 the heading instead of the first row of parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NdWvHTR47HJ",
    "outputId": "ee418d1b-d885-4198-9796-4ed0a8b09186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1     2     3\n",
      "0  22.01  0.72  1.21  0.61\n",
      "1  19.77  0.14  1.29  0.95\n",
      "2  24.64  1.32  1.21  0.66\n",
      "3  24.55  0.87  1.42  0.99\n",
      "4  21.79  0.51  1.26  0.76\n",
      "65611\n"
     ]
    }
   ],
   "source": [
    "df1.head() \n",
    "print(df1.head()) #gives first 5 rows\n",
    "print(df1.shape[0]) #prints number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zov3dON4OvIB",
    "outputId": "5f4c3105-0e05-40ca-d29e-fae6dd89b409"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1zGRT4aNV71ASxFXsG4MTZb0ZaqNslZSf #downloads real_t.csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mVMo7wsaOxC0"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(path + \"real_t.csv\", header=None)  #reads csv file; makes 0, 1, 2, 3 the heading instead of the first row of real_t values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUF6v_wpO28-",
    "outputId": "9dd61bdc-55c3-4353-d141-5cd93e4f3b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.923212 -0.913157 -0.901995 -0.889732 -0.876373 -0.861930 -0.846413   \n",
      "1 -0.150980 -0.107935 -0.064665 -0.021280  0.022109  0.065395  0.108474   \n",
      "2 -0.949474 -0.942679 -0.934842 -0.925965 -0.916052 -0.905110 -0.893145   \n",
      "3 -0.015974  0.031830  0.077765  0.121477  0.162622  0.200877  0.235939   \n",
      "4 -0.762562 -0.739172 -0.714570 -0.688787 -0.661854 -0.633810 -0.604694   \n",
      "\n",
      "        7         8         9    ...       291       292       293       294  \\\n",
      "0 -0.829833 -0.812206 -0.793548  ...  0.245705  0.230979  0.215836  0.200349   \n",
      "1  0.151243  0.193603  0.235461  ...  0.087291  0.242819  0.391866  0.527432   \n",
      "2 -0.880165 -0.866178 -0.851196  ...  0.825729  0.855296  0.881989  0.905727   \n",
      "3  0.267534  0.295418  0.319384  ...  0.435294  0.451729  0.466411  0.479366   \n",
      "4 -0.574548 -0.543418 -0.511353  ... -0.068997 -0.047779 -0.019230  0.016327   \n",
      "\n",
      "        295       296       297       298       299       300  \n",
      "0  0.184594  0.168656  0.152626  0.136604  0.120697  0.105025  \n",
      "1  0.643482  0.735414  0.800337  0.837132  0.846358  0.830031  \n",
      "2  0.926430  0.944015  0.958400  0.969504  0.977244  0.981539  \n",
      "3  0.490628  0.500236  0.508234  0.514671  0.519597  0.523061  \n",
      "4  0.058388  0.106278  0.159172  0.216121  0.276085  0.337978  \n",
      "\n",
      "[5 rows x 301 columns]\n",
      "65611\n"
     ]
    }
   ],
   "source": [
    "df2.head() \n",
    "print(df2.head()) #prints first 5 rows\n",
    "print(df2.shape[0]) #prints number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "lr = 1e-6\n",
    "# momentum = 0.9\n",
    "dropout_rate = 0.3\n",
    "td_ratio = 1-dropout_rate  # define percent of data that's training data\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSNSbu1QmTvR",
    "outputId": "c67aefe2-39fe-491a-d25d-5b3c41822251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full x data shape: torch.Size([65611, 4]) cuda:0\n",
      "x train data shape: torch.Size([46000, 1, 4]) cuda:0\n",
      "x test data shape: torch.Size([19611, 1, 4]) cuda:0\n",
      "y full data shape: torch.Size([65611, 301]) cuda:0\n",
      "y train data shape: torch.Size([46000, 1, 301]) cuda:0\n",
      "y test data shape: torch.Size([19611, 1, 301]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # Sets the seed for generating random numbers. will have the same random numbers each time\n",
    "\n",
    "# split the full data into training and test\n",
    "x_full=torch.tensor(df1.iloc[0:len(df1)].values).float().cuda()   #full x data from parameters.csv\n",
    "y_full=torch.tensor(df2.iloc[0:len(df2)].values).float().cuda()   #full y data from real_t\n",
    "\n",
    "inx=np.arange(len(df1)) # create array 0 to length(df1)-1\n",
    "np.random.shuffle(inx)   #randomly shuffle the index of the data\n",
    " \n",
    "dropout_rate = dropout_rate\n",
    "td_ratio = 1-dropout_rate #percent training data\n",
    "p= td_ratio  # define percent of data that's training data\n",
    "train_size= math.ceil(int(td_ratio*(len(df1)))/batch_size)*batch_size # size of training data\n",
    "# ^ prevent cases like 4 batches of 200 and one of 156, etc\n",
    "training_idx, test_idx = inx[:train_size], inx[train_size:] # index of training and test data\n",
    "x_train, x_test = x_full[training_idx,:], x_full[test_idx,:] #training x data and test data with shuffled inx \n",
    "y_train, y_test = y_full[training_idx,:], y_full[test_idx,:] #training y data and test data with shuffled inx \n",
    "\n",
    "#convert the data into N*1*4 shape or N*1*301 shape\n",
    "x_train=x_train.reshape(len(x_train),1,4)\n",
    "x_test=x_test.reshape(len(x_test),1,4)\n",
    "y_train=y_train.reshape(len(y_train),1,301)\n",
    "y_test=y_test.reshape(len(y_test),1,301)\n",
    "\n",
    "\n",
    "#print data shapes \n",
    "print('full x data shape:',x_full.shape, x_full.device)\n",
    "print('x train data shape:', x_train.shape, x_train.device)\n",
    "print('x test data shape:', x_test.shape, x_test.device)\n",
    "print('y full data shape:',  y_full.shape, y_full.device)\n",
    "print('y train data shape:', y_train.shape, y_train.device)\n",
    "print('y test data shape:', y_test.shape, y_test.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset class\n",
    "class SpectroData(Dataset):\n",
    "  def __init__(self, data): #load data and convert to tensors\n",
    "    self.data = data\n",
    "  def __getitem__(self, index): #gives data from imported dataset\n",
    "    return self.data[index] \n",
    "  def __len__(self):  #returns length of tensor\n",
    "    return len(self.data)\n",
    "#combine training data for dataloader\n",
    "train_dataset1 = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_dataset1= torch.utils.data.TensorDataset(x_test, y_test)  \n",
    "\n",
    "train_dataset=SpectroData(train_dataset1)\n",
    "test_dataset=SpectroData(test_dataset1)\n",
    "\n",
    "#load x_train and y_train together\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "# , num_workers=0, pin_memory=True) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XimLzBrcaUAD"
   },
   "outputs": [],
   "source": [
    "def f(x, W1,W2, V, b): #def bilinear function; x is a N*1*4\n",
    " Q=torch.randn(len(x),1,50).cuda() #Q is N*1*50  \n",
    " A = torch.randn(50,1).cuda() #define A\n",
    " F = torch.randn(50,1).cuda() #define F\n",
    " for i in range (0,len(x)): \n",
    "     e=torch.transpose(x[i],1,0).cuda() #transpose x from row to column (check) \n",
    "     et = torch.transpose(e,1,0).cuda()   #transpose e from row to column (check)\n",
    "     esq = torch.mul(e, e).cuda()  #squares e\n",
    "     for k in range (0, 50): \n",
    "         A[k, 0] = torch.mm(et,torch.mm(W1[k, :, :], e))  #puts values together -> 50x1 tensor\n",
    "         F[k, 0] =torch.mm(et,torch.mm(W2[k, :, :], esq))  #matrix multiplication\n",
    "     ecat = torch.cat((e, esq)) \n",
    "     L=torch.mm(V, ecat)\n",
    "     Q[i]= torch.transpose(A + F + L + b, 1,0) #adds all four 50x1 tensors -> output\n",
    " return(Q)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W1LTUxGx6Udq"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):   #https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(50, 4, 4)) #define W, tensor, 50 layers of 4x4 matrices\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(50, 4, 4)) #define W, tensor, 50 layers of 4x4 matrices\n",
    "        #W2 = torch.randn(50, 4, 4, requires_grad=True) #define W2\n",
    "        self.V = torch.nn.Parameter(torch.randn(50,8))    #define V\n",
    "        self.b = torch.nn.Parameter(torch.randn(50,1))    #define b        \n",
    "        self.linear_relu_stack = nn.Sequential(   #nn.Sequential -> output of each layer as input of next layer\n",
    "            nn.ReLU(),  #ReLU: activation function (defines output/what goes into next layer)\n",
    "            nn.Linear(50, 500), # creates single layer feed forward network with 50 inputs and 500 outputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 301),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x): #def forward function\n",
    "       Q=f(x, self.W1,self.W2,self.V, self.b)\n",
    "       logits = self.linear_relu_stack(Q)\n",
    "       return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJx4jSBktJ5o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46000, 1, 4])\n",
      "number of batches:  230\n",
      "loss at batch 1 tensor(160778.9531, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 11 tensor(138617.1875, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 21 tensor(142792.4062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 31 tensor(132587.3750, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 41 tensor(125679.5781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 51 tensor(125273.5156, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 61 tensor(133288.5312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 71 tensor(119682.4297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 81 tensor(130596.2891, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 91 tensor(109719.6562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 101 tensor(106692.6562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 111 tensor(111921.4453, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 121 tensor(114362.2656, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 131 tensor(92989.6641, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 141 tensor(100707.9453, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 151 tensor(96386.9922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 161 tensor(94046.8594, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 171 tensor(89823.6953, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 181 tensor(89717.1562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 191 tensor(88811.3203, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 201 tensor(78539.9531, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 211 tensor(79498.4922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 221 tensor(75743.4219, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2: [-677584.2841850462]\n",
      "trainloss: [110419.78125]\n",
      "loss at batch 1 tensor(80328.7812, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 11 tensor(69436.9062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 21 tensor(71698.6484, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 31 tensor(66731.1562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 41 tensor(63440.3633, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 51 tensor(63383.5742, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 61 tensor(67577.0312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 71 tensor(60814.4062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 81 tensor(66494.5859, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 91 tensor(55985.8867, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 101 tensor(54560.5195, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 111 tensor(57359.7500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 121 tensor(58726.3281, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 131 tensor(47797.4023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 141 tensor(51815.1289, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 151 tensor(49639.6797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 161 tensor(48472.2461, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 171 tensor(46327.0352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 181 tensor(46284.9922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 191 tensor(45854.4805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 201 tensor(40561.4961, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 211 tensor(41081.9844, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 221 tensor(39154.1719, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "epoch 1  loss_sum: tensor(56309.8047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2: [-677584.2841850462, -346109.65741182864]\n",
      "trainloss: [110419.78125, 56309.8046875]\n",
      "loss at batch 1 tensor(41551.7734, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 11 tensor(35933.0352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 21 tensor(37109.2070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 31 tensor(34529.2812, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 41 tensor(32837.1367, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 51 tensor(32802.9805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 61 tensor(34971.0391, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 71 tensor(31479.8984, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 81 tensor(34433.6719, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 91 tensor(29001.3652, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 101 tensor(28270.6445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 111 tensor(29733.3574, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 121 tensor(30457.9258, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 131 tensor(24800.0527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 141 tensor(26911.9844, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 151 tensor(25804.7656, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 161 tensor(25222.7969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 171 tensor(24131.3848, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 181 tensor(24127.0371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 191 tensor(23934.8848, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 201 tensor(21185.5488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 211 tensor(21470.3223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 221 tensor(20472.0762, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2: [-677584.2841850462, -346109.65741182864, -180155.9066685313]\n",
      "trainloss: [110419.78125, 56309.8046875, 29222.73828125]\n",
      "loss at batch 1 tensor(21730.1973, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 11 tensor(18797.1309, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 21 tensor(19418.2969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 31 tensor(18071.5488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 41 tensor(17200.7266, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 51 tensor(17188.3262, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 61 tensor(18329.5410, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 71 tensor(16508.2012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 81 tensor(18067.1348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 91 tensor(15219.6162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 101 tensor(14836.0732, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 111 tensor(15605.1406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 121 tensor(15983.8721, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 131 tensor(13010.5488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 141 tensor(14098.1221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 151 tensor(13495.4238, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 161 tensor(13175.4287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 171 tensor(12591.9014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 181 tensor(12574.0273, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 191 tensor(12465.1074, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 201 tensor(11018.7627, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 211 tensor(11151.6592, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 221 tensor(10622.5527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2: [-677584.2841850462, -346109.65741182864, -180155.9066685313, -94493.39346596872]\n",
      "trainloss: [110419.78125, 56309.8046875, 29222.73828125, 15287.5]\n",
      "loss at batch 1 tensor(11267.4824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 11 tensor(9742.2666, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 21 tensor(10052.2344, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 31 tensor(9331.8682, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 41 tensor(8869.4082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 51 tensor(8843.5322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 61 tensor(9411.2627, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 71 tensor(8464.6270, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 81 tensor(9254.5605, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 91 tensor(7782.6328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 101 tensor(7577.3179, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 111 tensor(7964.2651, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 121 tensor(8149.8330, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "loss at batch 131 tensor(6620.4375, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "print(x_train.shape)\n",
    "from torch.autograd import Variable  # Variable wraps tensor, gives way to perform backpropagation\n",
    "#x_train , y_train =(Variable(x_train),Variable(y_train))\n",
    "\n",
    "n_batches=math.ceil(len(x_train)/batch_size) # number of batches in train data\n",
    "print(\"number of batches: \", n_batches)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "loss = Variable(torch.FloatTensor(1)).zero_()\n",
    "loss.requires_grad = True\n",
    " \n",
    "\n",
    "Trainloss = [] # loss of every epoch in iteration for training data\n",
    "Testloss = [] #loss of every epoch in iteration for test data\n",
    "r_squared = []\n",
    "indices = []  #epochs\n",
    "\n",
    "y_train1=y_train.reshape(len(y_train),301).cuda()\n",
    "y_train2 = y_train1.detach().cuda()\n",
    "y_pred = []\n",
    "\n",
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "\n",
    "\n",
    "    i=0 # counter\n",
    "    for x_train_batch, y_train_batch in train_loader:\n",
    "        x_train_batch, y_train_batch =(Variable(x_train_batch),Variable(y_train_batch))\n",
    "        y_pred_batch = model(x_train_batch)\n",
    "       #combine all y_pred_batch into y_pred\n",
    "        if i==0: \n",
    "            y_pred=  y_pred_batch\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred, y_pred_batch),0)\n",
    "        i=i+1\n",
    "        loss =loss_func(y_train_batch, y_pred_batch)\n",
    "        if i%10==1:\n",
    "              print(\"loss at batch\",i ,loss)\n",
    "        optimizer.zero_grad()\n",
    "#     loss.requires_grad = True\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    loss_sum =loss_func(y_pred, y_train.cuda())\n",
    "    if epoch%5==1:\n",
    "        print(\"epoch\", epoch,  \" loss_sum:\", loss_sum)\n",
    "#r-squared score\n",
    "    y_pred1=y_pred.reshape(len(y_train),301)\n",
    "    y_pred2 = y_pred1.detach()\n",
    "    r2=r2_score(y_train2.cpu(), y_pred2.cpu())\n",
    "    r_squared.append(r2)\n",
    "    print(\"r2:\", r_squared)\n",
    "    \n",
    "# Compute and print loss\n",
    "    Trainloss.append(loss_sum.item())\n",
    "    print(\"trainloss:\", Trainloss)\n",
    "    indices.append(epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE=\"model_feb27.pth\"\n",
    "torch.save(model, FILE)\n",
    "model_save=torch.load(FILE)\n",
    "model_save.eval()\n",
    "for param in model_save.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"./savedmodel.pth\"           # save the trained model\n",
    "torch.save(model.state_dict(),save_path )\n",
    "device=torch.device(\"cuda\")\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(save_path))# load the trained model \n",
    "model.to(device)\n",
    "model.eval() \n",
    "y_pred_train=[]\n",
    "i=0 \n",
    "for x_train_batch, y_train_batch in train_loader:\n",
    "         x_train_batch, y_train_batch =(Variable(x_train_batch),Variable(y_train_batch))\n",
    "         y_pred_batch = model(x_train_batch)\n",
    "         #print(y_pred_batch)\n",
    "         # combine all y_pred_batch into y_pred\n",
    "         if i==0: \n",
    "            y_pred_train=  y_pred_batch\n",
    "         else:\n",
    "            y_pred_train = torch.cat((y_pred_train, y_pred_batch),0)\n",
    "         i=i+1\n",
    "     # prediction_train = model(x_train)\n",
    "loss_train = loss_func(y_pred_train,y_train.cuda())\n",
    "print(loss_train)\n",
    "    \n",
    "    \n",
    "y_pred_test=[]\n",
    "i=0 \n",
    "for x_test_batch, y_test_batch in test_loader:\n",
    "         x_test_batch, y_test_batch =(Variable(x_test_batch),Variable(y_test_batch))\n",
    "         y_pred_batch = model(x_test_batch)\n",
    "         #print(y_pred_batch)\n",
    "         # combine all y_pred_batch into y_pred\n",
    "         if i==0: \n",
    "            y_pred_test=  y_pred_batch\n",
    "         else:\n",
    "            y_pred_test = torch.cat((y_pred_test, y_pred_batch),0)\n",
    "         i=i+1\n",
    "     # prediction_train = model(x_train)\n",
    "loss_test = loss_func(y_pred_test,y_test.cuda())\n",
    "print(loss_test)\n",
    "# with torch.no_grad():\n",
    "#      prediction_train = model(x_train)\n",
    "#      loss = loss_func(prediction_train,y_train)\n",
    "#      print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_test=[]\n",
    "\n",
    "for epoch in range(100,102):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "\n",
    "\n",
    "    i=0 # counter\n",
    "    for x_train_batch, y_train_batch in train_loader:\n",
    "        x_train_batch, y_train_batch =(Variable(x_train_batch),Variable(y_train_batch))\n",
    "        y_pred_batch = model(x_train_batch)\n",
    "       #combine all y_pred_batch into y_pred\n",
    "        if i==0: \n",
    "            y_pred=  y_pred_batch\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred, y_pred_batch),0)\n",
    "        i=i+1\n",
    "        loss =loss_func(y_train_batch, y_pred_batch)\n",
    "        if i%10==1:\n",
    "              print(\"train loss at batch\",i ,loss)\n",
    "        optimizer.zero_grad()\n",
    "#     loss.requires_grad = True\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    loss_sum =loss_func(y_pred, y_train.cuda())\n",
    "    print( \"epoch\", epoch, \" train loss: \" , loss_sum)\n",
    "        \n",
    "        \n",
    "        \n",
    "    j=0 \n",
    "    for x_test_batch, y_test_batch in test_loader:\n",
    "         x_test_batch, y_test_batch =(Variable(x_test_batch),Variable(y_test_batch))\n",
    "         y_pred_batch = model(x_test_batch)\n",
    "         #print(y_pred_batch)\n",
    "         # combine all y_pred_batch into y_pred\n",
    "         if j==0: \n",
    "            y_pred_test=  y_pred_batch\n",
    "         else:\n",
    "            y_pred_test = torch.cat((y_pred_test, y_pred_batch),0)\n",
    "         j=j+1\n",
    "     # prediction_train = model(x_train)\n",
    "    loss_test = loss_func(y_pred_test,y_test.cuda())\n",
    "    print( \"epoch\", epoch, \" test loss: \" , loss_test)\n",
    " #r-squared score\n",
    "    y_pred1=y_pred.reshape(len(y_train),301)\n",
    "    y_pred2 = y_pred1.detach()\n",
    "    r2=r2_score(y_train2.cpu(), y_pred2.cpu())\n",
    "    r_squared.append(r2)\n",
    "    print( \"epoch\", epoch, \" r2: \" , r_squared)\n",
    "    \n",
    "# Compute and print loss\n",
    "    Trainloss.append(loss_sum.item())\n",
    "    print(\"trainloss:\", Trainloss)\n",
    "    Testloss.append(loss_test.item())\n",
    "    indices.append(epoch+1)\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ibeomImPecL"
   },
   "outputs": [],
   "source": [
    "save_path=\"./savedmodel.pth\"           # save the trained model\n",
    "torch.save(model.state_dict(),save_path )\n",
    "device=torch.device(\"cuda\")\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(save_path))# load the trained model \n",
    "model.to(device)\n",
    "model.eval() \n",
    "y_pred_train=[]\n",
    "with torch.no_grad():\n",
    "    i=0 \n",
    "    for x_train_batch, y_train_batch in train_loader:\n",
    "         x_train_batch, y_train_batch =(Variable(x_train_batch),Variable(y_train_batch))\n",
    "         y_pred_batch = model(x_train_batch)\n",
    "         #print(y_pred_batch)\n",
    "         # combine all y_pred_batch into y_pred\n",
    "         if i==0: \n",
    "            y_pred_train=  y_pred_batch\n",
    "         else:\n",
    "            y_pred_train = torch.cat((y_pred_train, y_pred_batch),0)\n",
    "         i=i+1\n",
    "     # prediction_train = model(x_train)\n",
    "    loss_train = loss_func(y_pred_train,y_train.cuda())\n",
    "    print(loss_train)\n",
    "    \n",
    "    \n",
    "    y_pred_test=[]\n",
    "    i=0 \n",
    "    for x_test_batch, y_test_batch in test_loader:\n",
    "         x_test_batch, y_test_batch =(Variable(x_test_batch),Variable(y_test_batch))\n",
    "         y_pred_batch = model(x_test_batch)\n",
    "         #print(y_pred_batch)\n",
    "         # combine all y_pred_batch into y_pred\n",
    "         if i==0: \n",
    "            y_pred_test=  y_pred_batch\n",
    "         else:\n",
    "            y_pred_test = torch.cat((y_pred_test, y_pred_batch),0)\n",
    "         i=i+1\n",
    "     # prediction_train = model(x_train)\n",
    "    loss_test = loss_func(y_pred_test,y_test.cuda())\n",
    "    print(loss_test)\n",
    "# with torch.no_grad():\n",
    "#      prediction_train = model(x_train)\n",
    "#      loss = loss_func(prediction_train,y_train)\n",
    "#      print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-njG7fALOQoT"
   },
   "outputs": [],
   "source": [
    "plt.plot(indices, '-o')\n",
    "plt.plot(Trainloss, '-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Loss','Test Loss'])\n",
    "plt.title('Epochs vs Loss')\n",
    " \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-Z7oZ5E143e"
   },
   "outputs": [],
   "source": [
    "plt.plot(r_squared, '-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R-squared value')\n",
    "plt.title('Epoch vs R-squared val')\n",
    "# plt.legend(['p'])\n",
    "\n",
    "r2_value=r_squared[5:100]\n",
    "x_epoch=list(range(5,100))\n",
    "plt.plot(x_epoch,r2_value, '-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R-squared value')\n",
    "plt.title('Epoch vs R-squared val')\n",
    "# plt.legend(['p'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "print(loss_test)\n",
    "loss_train=Trainloss[k:100]\n",
    "loss_test=loss_test[k:100]\n",
    "x_epoch=list(range(k,100))\n",
    "plt.plot(x_epoch,loss_train, '-o')\n",
    "plt.plot(x_epoch,loss_test, '-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Loss','Test Loss'])\n",
    "plt.title('Epochs vs Loss')\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "r2_value=r_squared[80:100]\n",
    "x_epoch=list(range(80,100))\n",
    "plt.plot(x_epoch,r2_value, '-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R-squared value')\n",
    "plt.title('Epoch vs R-squared val')\n",
    "# plt.legend(['p'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "current code: nn-13.0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
